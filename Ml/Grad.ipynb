{"cells":[{"cell_type":"markdown","source":["# MNIST Database MLP optimization with Numerical Methods!"],"metadata":{"id":"r-QHhKKsgeLe"}},{"cell_type":"markdown","source":["## Getting the data from the Built in MNIST Data set in tensorflow Module"],"metadata":{"id":"8CDzH2jLgueI"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","\n","\n","def load_and_preprocess_mnist():\n","    # Carregando as Imagens\n","    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()   # Dataset do Tensorflow\n","\n","    # Normalizando as Imagens...\n","    threshold = 128\n","    train_images_flattened = np.where(train_images > threshold, 1, 0).reshape(train_images.shape[0], -1)\n","    test_images_flattened = np.where(test_images > threshold, 1, 0).reshape(test_images.shape[0], -1)\n","\n","    return (train_images_flattened, train_labels)  # Retornando array Principal\n","\n","# Exemplo de Uso...\n","(train_images_flattened, train_labels) = load_and_preprocess_mnist()\n","\n","\n"],"metadata":{"id":"FUqllZsIgs7h","executionInfo":{"status":"ok","timestamp":1717621101419,"user_tz":180,"elapsed":5734,"user":{"displayName":"ian bezerra","userId":"05417333999231090698"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8da797af-7f05-424b-ff1f-30af9f244f2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 2s 0us/step\n"]}]},{"cell_type":"code","source":["\n","# Example: Print the first flattened image and its label\n","#print(\"First training image (flattened):\", train_images_flattened[200])\n","#n = int(input(\"Enter a number of the index up to 60k: \"))\n","#print(\"Label of the first training image:\", train_labels[n])\n","print(\"----------------------------------------\")\n","#print(f\"The data set is {len(train_images_flattened)} long:\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"194QtatkhC4Q","executionInfo":{"status":"ok","timestamp":1717621101420,"user_tz":180,"elapsed":5,"user":{"displayName":"ian bezerra","userId":"05417333999231090698"}},"outputId":"b4638a7d-625b-4bc8-8534-041c6b7f1acc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------\n"]}]},{"cell_type":"markdown","source":["# ---------------------------------------------------------------------"],"metadata":{"id":"4iI3y_ijg3x_"}},{"cell_type":"markdown","source":["## Making a Pygame Grid That Reconstruct the Image!"],"metadata":{"id":"58WYTuFQhSIv"}},{"cell_type":"markdown","source":["# ---------------------------------------------------------------------"],"metadata":{"id":"ypAbCGHYhhzY"}},{"cell_type":"code","source":["\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Define a simple neural network\n","class SimpleNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleNN, self).__init__()\n","        self.fc1 = nn.Linear(2, 5)\n","        self.fc2 = nn.Linear(5, 1)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Generate some random data\n","torch.manual_seed(0)\n","X = torch.randn(100, 2)\n","y = torch.randn(100, 1)\n","\n","# Initialize the model\n","model = SimpleNN()\n","\n","# Define the loss function\n","criterion = nn.crossLoss()\n","\n","# Custom Conjugate Gradient optimizer\n","class ConjugateGradientOptimizer:\n","    def __init__(self, params, lr=0.01):\n","        self.params = list(params)\n","        self.lr = lr\n","        self.reset_state()\n","\n","    def reset_state(self):\n","        self.old_grads = None\n","        self.old_p = None\n","\n","    def zero_grad(self):\n","        for p in self.params:\n","            if p.grad is not None:\n","                p.grad.zero_()\n","\n","    def step(self, closure):\n","        loss = closure()\n","        grads = [p.grad.data.clone() for p in self.params]\n","\n","        if self.old_grads is None:\n","            self.old_grads = grads\n","            self.old_p = grads\n","        else:\n","            denom = sum((og * og).sum() for og in self.old_grads)\n","            if denom == 0:\n","                beta = 0\n","            else:\n","                beta = sum((g * g).sum() for g in grads) / denom\n","            self.old_p = [g + beta * op for g, op in zip(grads, self.old_p)]\n","            self.old_grads = grads\n","\n","        # Gradient Clipping\n","        torch.nn.utils.clip_grad_norm_(self.params, max_norm=1.0)\n","\n","        for p, d in zip(self.params, self.old_p):\n","            p.data = p.data - self.lr * d\n","\n","        return loss\n","\n","# Instantiate the optimizer\n","optimizer = ConjugateGradientOptimizer(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    def closure():\n","        optimizer.zero_grad()\n","        outputs = model(X)\n","        loss = criterion(outputs, y)\n","        loss.backward()\n","        return loss\n","\n","    loss = optimizer.step(closure)\n","    if epoch % 10 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","print(\"Training complete.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2_Aa8Xi2wCH","executionInfo":{"status":"ok","timestamp":1717635358895,"user_tz":180,"elapsed":1405,"user":{"displayName":"ian bezerra","userId":"05417333999231090698"}},"outputId":"6881a03a-41a7-40dd-aded-240732a0102a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/1000], Loss: 0.9897\n","Epoch [11/1000], Loss: 0.9838\n","Epoch [21/1000], Loss: 0.9752\n","Epoch [31/1000], Loss: 0.9681\n","Epoch [41/1000], Loss: 0.9607\n","Epoch [51/1000], Loss: 0.9533\n","Epoch [61/1000], Loss: 0.9478\n","Epoch [71/1000], Loss: 0.9437\n","Epoch [81/1000], Loss: 0.9402\n","Epoch [91/1000], Loss: 0.9364\n","Epoch [101/1000], Loss: 0.9329\n","Epoch [111/1000], Loss: 0.9306\n","Epoch [121/1000], Loss: 0.9292\n","Epoch [131/1000], Loss: 0.9279\n","Epoch [141/1000], Loss: 0.9266\n","Epoch [151/1000], Loss: 0.9253\n","Epoch [161/1000], Loss: 0.9244\n","Epoch [171/1000], Loss: 0.9233\n","Epoch [181/1000], Loss: 0.9220\n","Epoch [191/1000], Loss: 0.9204\n","Epoch [201/1000], Loss: 0.9193\n","Epoch [211/1000], Loss: 0.9185\n","Epoch [221/1000], Loss: 0.9177\n","Epoch [231/1000], Loss: 0.9167\n","Epoch [241/1000], Loss: 0.9157\n","Epoch [251/1000], Loss: 0.9150\n","Epoch [261/1000], Loss: 0.9144\n","Epoch [271/1000], Loss: 0.9136\n","Epoch [281/1000], Loss: 0.9126\n","Epoch [291/1000], Loss: 0.9119\n","Epoch [301/1000], Loss: 0.9114\n","Epoch [311/1000], Loss: 0.9107\n","Epoch [321/1000], Loss: 0.9108\n","Epoch [331/1000], Loss: 1.0979\n","Epoch [341/1000], Loss: nan\n","Epoch [351/1000], Loss: nan\n","Epoch [361/1000], Loss: nan\n","Epoch [371/1000], Loss: nan\n","Epoch [381/1000], Loss: nan\n","Epoch [391/1000], Loss: nan\n","Epoch [401/1000], Loss: nan\n","Epoch [411/1000], Loss: nan\n","Epoch [421/1000], Loss: nan\n","Epoch [431/1000], Loss: nan\n","Epoch [441/1000], Loss: nan\n","Epoch [451/1000], Loss: nan\n","Epoch [461/1000], Loss: nan\n","Epoch [471/1000], Loss: nan\n","Epoch [481/1000], Loss: nan\n","Epoch [491/1000], Loss: nan\n","Epoch [501/1000], Loss: nan\n","Epoch [511/1000], Loss: nan\n","Epoch [521/1000], Loss: nan\n","Epoch [531/1000], Loss: nan\n","Epoch [541/1000], Loss: nan\n","Epoch [551/1000], Loss: nan\n","Epoch [561/1000], Loss: nan\n","Epoch [571/1000], Loss: nan\n","Epoch [581/1000], Loss: nan\n","Epoch [591/1000], Loss: nan\n","Epoch [601/1000], Loss: nan\n","Epoch [611/1000], Loss: nan\n","Epoch [621/1000], Loss: nan\n","Epoch [631/1000], Loss: nan\n","Epoch [641/1000], Loss: nan\n","Epoch [651/1000], Loss: nan\n","Epoch [661/1000], Loss: nan\n","Epoch [671/1000], Loss: nan\n","Epoch [681/1000], Loss: nan\n","Epoch [691/1000], Loss: nan\n","Epoch [701/1000], Loss: nan\n","Epoch [711/1000], Loss: nan\n","Epoch [721/1000], Loss: nan\n","Epoch [731/1000], Loss: nan\n","Epoch [741/1000], Loss: nan\n","Epoch [751/1000], Loss: nan\n","Epoch [761/1000], Loss: nan\n","Epoch [771/1000], Loss: nan\n","Epoch [781/1000], Loss: nan\n","Epoch [791/1000], Loss: nan\n","Epoch [801/1000], Loss: nan\n","Epoch [811/1000], Loss: nan\n","Epoch [821/1000], Loss: nan\n","Epoch [831/1000], Loss: nan\n","Epoch [841/1000], Loss: nan\n","Epoch [851/1000], Loss: nan\n","Epoch [861/1000], Loss: nan\n","Epoch [871/1000], Loss: nan\n","Epoch [881/1000], Loss: nan\n","Epoch [891/1000], Loss: nan\n","Epoch [901/1000], Loss: nan\n","Epoch [911/1000], Loss: nan\n","Epoch [921/1000], Loss: nan\n","Epoch [931/1000], Loss: nan\n","Epoch [941/1000], Loss: nan\n","Epoch [951/1000], Loss: nan\n","Epoch [961/1000], Loss: nan\n","Epoch [971/1000], Loss: nan\n","Epoch [981/1000], Loss: nan\n","Epoch [991/1000], Loss: nan\n","Training complete.\n"]}]},{"cell_type":"code","source":["# Testando\n","image = xs[0] # Primeira Imagem do banco de Dados\n","predicted_digit = predict_digit(image, trained_model)\n","print(f'Predicted Digit: {predicted_digit}')\n","print(f'Predicted Digit: {train_labels[0]}')"],"metadata":{"id":"zJGTJLksJ5Zi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'mnist_model.pth')"],"metadata":{"id":"JwZpRv4T6R0a"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}